{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWPnYnnh1+DTbkdGrOsjp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeesem/Deep_Learning/blob/main/Building_deep_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h4DMKI0gwXmD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import copy\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "yCMxvax8wfSJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "VnfHjZ8pw4X0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "#     Arguments:\n",
        "#     A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "#     W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "#     b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "def linear_forward(A, W, b):\n",
        "\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "Nnt5mAnaxA-6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "  A = np.maximum(0,Z)\n",
        "  activation_cache = (Z,)\n",
        "  return A,activation_cache"
      ],
      "metadata": {
        "id": "hJZMobxLxnos"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "  A = 1 / (1 + np.exp(-Z))\n",
        "  activation_cache = (Z,)\n",
        "  return A,activation_cache"
      ],
      "metadata": {
        "id": "IZ37v01Hzkn1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A,activation_cache = relu(Z)\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "L7aAMuTdztID"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "    Returns:\n",
        "    AL -- activation value from the output (last) layer\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "\n",
        "    #The \"//\" operator in Python is the floor division operator.\n",
        "    #It performs division and rounds down to the nearest integer\n",
        "    L = len(parameters) // 2  # number of layers in the neural network\n",
        "\n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    # The for loop starts at 1 because layer 0 is the input\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Update A_prev for the final layer\n",
        "    A_prev = A\n",
        "\n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL, caches\n"
      ],
      "metadata": {
        "id": "jCij3vitz2Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    cost = -1/m * np.sum(Y*np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
        "\n",
        "    # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "o4HlBrAs3cKe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "print('axis=1 and keepdims=True')\n",
        "print(np.sum(A, axis=1, keepdims=True))\n",
        "print('axis=1 and keepdims=False')\n",
        "print(np.sum(A, axis=1, keepdims=False))\n",
        "print('axis=0 and keepdims=True')\n",
        "print(np.sum(A, axis=0, keepdims=True))\n",
        "print('axis=0 and keepdims=False')\n",
        "print(np.sum(A, axis=0, keepdims=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXE-k9S44D6t",
        "outputId": "5b5a70b3-3826-4a7e-8f71-575faaeb3afd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "axis=1 and keepdims=True\n",
            "[[3]\n",
            " [7]]\n",
            "axis=1 and keepdims=False\n",
            "[3 7]\n",
            "axis=0 and keepdims=True\n",
            "[[4 6]]\n",
            "axis=0 and keepdims=False\n",
            "[4 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, activation_cache):\n",
        "\n",
        "    Z = activation_cache\n",
        "\n",
        "    dZ = np.multiply(dA, np.int64(Z > 0))\n",
        "    # Element-wise multiplication with the ReLU derivative\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "yNW3AOsd-u57"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "\n",
        "    Z = cache\n",
        "\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "\n",
        "    # The derivative of the sigmoid function\n",
        "    dZ = dA * s * (1 - s)\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "T6xTMdUc-6gG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "\n",
        "    dW = 1/m * np.dot(dZ,A_prev.T)\n",
        "    db = 1/m * np.sum(dZ,axis = 1,keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "KIfpfn-8_Jz0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "\n",
        "        dZ = relu_backward(dA,activation_cache)\n",
        "        dA_prev,dW,db = linear_backward(dZ, linear_cache)\n",
        "        # YOUR CODE ENDS HERE\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "\n",
        "        dZ = sigmoid_backward(dA,activation_cache)\n",
        "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "V7yYQqL3463D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    dAL = -(np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp,dW_temp,db_temp = linear_activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "\n",
        "    for l in reversed(range(L-1)):\n",
        "\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp,dW_temp,db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)],current_cache,activation = \"relu\")\n",
        "        grads[\"dA\"+str(l)] = dA_prev_temp\n",
        "        grads[\"dW\"+str(l+1)] = dW_temp\n",
        "        grads[\"db\"+str(l+1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "Yiggv7GwL6b0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    params -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    parameters = copy.deepcopy(params)\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "W16_KoN-_OgV"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}